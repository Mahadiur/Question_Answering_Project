{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": "### **Question Answering Project Using Fine-Tune LLM Model**",
   "id": "6dc6b23b07efb12"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T02:17:21.343098Z",
     "start_time": "2025-10-29T02:17:21.340122Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Any\n",
    "\n",
    "from databricks.sdk import DatabaseAPI\n",
    "from pytorch_lightning.utilities.types import STEP_OUTPUT\n",
    "\n",
    "''' Import all import Libraries '''\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pytorch_lightning\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering"
   ],
   "id": "6924d75253f2cb45",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T02:17:21.350895Z",
     "start_time": "2025-10-29T02:17:21.348551Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Device: {Device}')"
   ],
   "id": "7d04d603cbe56d5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T02:17:21.360327Z",
     "start_time": "2025-10-29T02:17:21.358423Z"
    }
   },
   "cell_type": "code",
   "source": [
    "''' Dataset Path '''\n",
    "Root_dir = '/Users/mahadiur/Desktop/Bongodev MLops Projects/Question Answering Using Fine-Tune LLM Model/Data'\n",
    "\n",
    "test_path = os.path.join(Root_dir, 'qna_test.csv')\n",
    "train_path = os.path.join(Root_dir, 'qna_train.csv')"
   ],
   "id": "fd61445d62303a0d",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T02:17:21.727711Z",
     "start_time": "2025-10-29T02:17:21.367307Z"
    }
   },
   "cell_type": "code",
   "source": [
    "''' Load Dataset '''\n",
    "train_dataset = pd.read_csv(train_path, encoding='Iso-8859-1', on_bad_lines='skip', engine='python')\n",
    "test_data = pd.read_csv(test_path, encoding='ISO-8859-1', on_bad_lines='skip', engine='python')\n",
    "\n",
    "train_dataset.head()"
   ],
   "id": "940e314cb7ed2e31",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                             context  \\\n",
       "0  Architecturally, the school has a Catholic cha...   \n",
       "1  Architecturally, the school has a Catholic cha...   \n",
       "2  Architecturally, the school has a Catholic cha...   \n",
       "3  Architecturally, the school has a Catholic cha...   \n",
       "4  Architecturally, the school has a Catholic cha...   \n",
       "\n",
       "                                            question  \\\n",
       "0  To whom did the Virgin Mary allegedly appear i...   \n",
       "1  What is in front of the Notre Dame Main Building?   \n",
       "2  The Basilica of the Sacred heart at Notre Dame...   \n",
       "3                  What is the Grotto at Notre Dame?   \n",
       "4  What sits on top of the Main Building at Notre...   \n",
       "\n",
       "                                             answers  \n",
       "0  {'text': ['Saint Bernadette Soubirous'], 'answ...  \n",
       "1  {'text': ['a copper statue of Christ'], 'answe...  \n",
       "2  {'text': ['the Main Building'], 'answer_start'...  \n",
       "3  {'text': ['a Marian place of prayer and reflec...  \n",
       "4  {'text': ['a golden statue of the Virgin Mary'...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
       "      <td>{'text': ['Saint Bernadette Soubirous'], 'answ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What is in front of the Notre Dame Main Building?</td>\n",
       "      <td>{'text': ['a copper statue of Christ'], 'answe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
       "      <td>{'text': ['the Main Building'], 'answer_start'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What is the Grotto at Notre Dame?</td>\n",
       "      <td>{'text': ['a Marian place of prayer and reflec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What sits on top of the Main Building at Notre...</td>\n",
       "      <td>{'text': ['a golden statue of the Virgin Mary'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **Download Hugging face model & Tokenizer**",
   "id": "c3355a77f68340ae"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T02:19:41.061734Z",
     "start_time": "2025-10-29T02:17:21.744993Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "Fine_Tune_Model_name = 'deepset/roberta-base-squad2'\n",
    "\n",
    "Fine_Tune_Model = AutoModelForQuestionAnswering.from_pretrained(Fine_Tune_Model_name)\n",
    "Tokenizer = AutoTokenizer.from_pretrained(Fine_Tune_Model_name)"
   ],
   "id": "ea89e1e9fdcd4e0d",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **Explore Dataset**",
   "id": "d43aae67082341e1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T02:24:47.756719Z",
     "start_time": "2025-10-29T02:24:47.752789Z"
    }
   },
   "cell_type": "code",
   "source": [
    "''' Dataset column '''\n",
    "train_dataset.columns"
   ],
   "id": "39e6ded53330d64d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['context', 'question', 'answers'], dtype='object')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T02:27:23.915862Z",
     "start_time": "2025-10-29T02:27:23.913481Z"
    }
   },
   "cell_type": "code",
   "source": [
    "idx = 0\n",
    "context = train_dataset.iloc[idx]['context']\n",
    "print(context)"
   ],
   "id": "11dfbbf789e3fc8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T02:28:52.280960Z",
     "start_time": "2025-10-29T02:28:52.276689Z"
    }
   },
   "cell_type": "code",
   "source": [
    "question = train_dataset.iloc[idx]['question']\n",
    "print(question)"
   ],
   "id": "a9dcad0840a3e576",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T02:33:02.108883Z",
     "start_time": "2025-10-29T02:33:02.105521Z"
    }
   },
   "cell_type": "code",
   "source": [
    "answers = eval(train_dataset.iloc[idx]['answers'])\n",
    "print(answers['text'])\n",
    "print(answers['answer_start'])"
   ],
   "id": "9bac4a5f6d2c7b29",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Saint Bernadette Soubirous']\n",
      "[515]\n"
     ]
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **Data (QnA Part 1)**",
   "id": "729718bb1a4bddf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T02:37:32.927528Z",
     "start_time": "2025-10-29T02:37:32.923186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Dataset class\n",
    "class QnADataset(Dataset):\n",
    "    # Dataset\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        pass\n",
    "    # Find length of dataset\n",
    "    def __len__(self):\n",
    "        pass\n",
    "    # Ready a Single example\n",
    "    def __getitem__(self, idx):\n",
    "        pass\n"
   ],
   "id": "3d0507d795306601",
   "outputs": [],
   "execution_count": 74
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Data Module Class\n",
    "class QnADataModule(pytorch_lightning.LightningDataModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        pass\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        pass\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        pass\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        pass\n"
   ],
   "id": "7cdd2f1916c397bf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "Data_Module = QnADataModule()",
   "id": "a4fd4c89b5b72e4f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **Model (QnA Part 2)**",
   "id": "8ebf1090d390b4fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Model\n",
    "class QnAModel(pytorch_lightning.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        pass\n",
    "\n",
    "    def forward(self):\n",
    "        pass\n",
    "\n",
    "    def compute_loss(self, batch):\n",
    "        pass\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        pass\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        pass\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        pass\n"
   ],
   "id": "bbf29735ddbebeb7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "Model = QnAModel()",
   "id": "59c3068cd0b32aea"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **Training (QnA Part 3)**",
   "id": "ddf853b74b0ededa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "Training = pytorch_lightning.Trainer( max_epochs=10, gpus=1)",
   "id": "13453c24e3979541"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "Training.fit(Model, Data_Module)",
   "id": "f89b9a9756a3499a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "Training.test(Model, Data_Module)",
   "id": "6e64837df21d582b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
